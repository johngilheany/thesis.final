{
    "collab_server" : "",
    "contents" : "# Logistic Regression Model \nOnce the final data set was created and cleaned, with a number of response variables including trailing beta, trailing volatility, price to book ratio, and the whether or not a stock was in the Minimum Volatility index or not (1 if in, 0 if not in). Given the nature of the data, a logistic regression was run. Looking at all of the historical data and stock various characteristics, this  modeled the log odds of a stock being in the minimum volatility index as a combination of the linear predictors mentioned. Since the index is rebalanced twice a year, it makes sense to look at a model including the data for both of these months.\n\n## Data Cleaning\n```{r data_cleaning, echo = FALSE}\nload(\"~/thesis_final/data/monthly_data.Rda\")\n# Subset data for dates from May\nmay_final <- filter(monthly_data, date == \"2012-05-31\" |  date == \"2013-05-31\"| date == \"2014-05-30\" | date == \"2015-05-29\" | date == \"2016-05-31\")\n# Remove NA values from set\nmay_final <- subset(may_final, !is.na(index_before))\n# Subset data for dates from November\nnovember_final <- filter(monthly_data, date == \"2011-11-30\" | date == \"2012-11-30\"| date == \"2013-11-29\"| date == \"2014-11-28\" | date == \"2015-11-30\" | date == \"2016-11-30\")\n# Remove NA values from set\nnovember_final <- subset(november_final, !is.na(index_before))\n\nboth_final <- rbind(may_final, november_final)\n```\n### Removing for Class Bias\nIdeally, the proportion of stocks in and out of the Min Vol index should approximately be the same. However, after checking this, it is clear that this is not the case, as, just around 25% of the data is from stocks that are currently in the index, meaning there is a class bias. As a result, observations must be sampled in approximately equal proportions to get a better model.\n```{r}\ntable(both_final$index_now)\n```\n### Create Training and Test Samples\nOne way to address the problem of class bias is to draw the 0’s and 1’s for the trainingData (development sample) in equal proportions. In doing so, we will put rest of the inputData not included for training into testData (validation sample). As a result, the size of development sample will be smaller that validation, which is okay, because, there are large number of observations.\n```{r data_trans, echo=FALSE}\n# Create Training Data\ninput_ones2 <- both_final[which(both_final$index_now == 1), ] \ninput_zeros2 <- both_final[which(both_final$index_now == 0), ]\nset.seed(100)  # for repeatability of samples\ninput_ones_training_rows2 <- sample(1:nrow(input_ones2), 0.7*nrow(input_ones2)) \ninput_zeros_training_rows2 <- sample(1:nrow(input_zeros2), 0.7*nrow(input_ones2))  \ntraining_ones2 <- input_ones2[input_ones_training_rows2, ]  \ntraining_zeros2 <- input_zeros2[input_zeros_training_rows2, ]\ntrainingData <- rbind(training_ones2, training_zeros2)   \n# Create Test Data\ntest_ones2 <- input_ones2[-input_ones_training_rows2, ]\ntest_zeros2 <- input_zeros2[-input_zeros_training_rows2, ]\ntestData <- rbind(test_ones2, test_zeros2) \n```\nThere is no more class bias, as the sample is now evenly weighted now, with each outcome being represented by 969 observations. \n```{r}\ntable(trainingData$index_now)\n```\n\n### Logistic Regression Model \n```{r}\n# Model\nlogit <- glm(index_now ~  volatility + beta + price_to_book \n+ index_before, data=trainingData, family=binomial(link=\"logit\"))\n\n# Summary of Model\nsummary(logit)\n\n# Coefficient Interpretation\n## Log Odds\nexp(coef(logit))\n## Probability \n(exp(coef(logit))) / (1+(exp(coef(logit))))\n``` \n\n### Interpretation of Model\nThe coefficients can be interpreted as: \n\\hfill\\break\n- Volatility: The odds ratio of being added to the index is 0.96 times smaller, given a one unit increase in volatility. This response variable is not statistically significant.\n\\hfill\\break\n- Beta: The odds ratio of being added to the index is 0.015 times smaller, given a one unit increase in beta. This response variable is statistically significant. \n\\hfill\\break\n- Price to Book: The odds ratio of being added to the index is the same, given a one unit increase in price to book ratio. This response variable is not statistically significant. \n\\hfill\\break\n- Index before: The odds ratio of being added to the index is 228.61 times greater if the stock was in the index 6 months ago. This response variable is statistically significant. \n\\hfill\\break\n\\hfill\\break\n\n### Model Quality \nTo test the quality of the model, several tests were done:\n\\hfill\\break\n__Predictive Power__\n\\hfill\\break\nThe default cutoff prediction probability score is 0.5 or the ratio of 1’s and 0’s in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The InformationValue::optimalCutoff function provides ways to find the optimal cutoff to improve the prediction of 1’s, 0’s, both 1’s and 0’s and to reduce the misclassification error. Here, the optimal cut off is 0.86.\n```{r}\nlibrary(InformationValue)\npredicted <- plogis(predict(logit, testData)) \noptCutOff <- optimalCutoff(testData$index_now, predicted)[1] \noptCutOff\n```\n\\hfill\\break\n__VIF__\n\\hfill\\break\nLike in case of linear regression, we should check for multicollinearity in the model. As seen below, all the variables in the model have VIF well below 4.\n```{r, message=FALSE}\nlibrary(car)\nvif(logit)\n```\n\\hfill\\break\n__Misclassification Error__\n\\hfill\\break\nMisclassification error is the percentage mismatch of predicted vs. actuals, irrespective of 1’s or 0’s. The lower the misclassification error, the better the model. Here it is 4.02%.\n```{r}\nmisClassError(testData$index_now, predicted)\n```\n\\hfill\\break\n__ROC__\n\\hfill\\break\nReceiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. For a good model, as the cutoff is lowered, it should mark more of actual 1’s as positives and lesser of actual 0’s as 1’s. So for a good model, the curve should rise steeply, indicating that the TPR (Y-Axis) increases faster than the FPR (X-Axis) as the cutoff score decreases. Greater the area under the ROC curve, better the predictive ability of the model. Here, it is 96.77%.\n```{r}\nplotROC(testData$index_now, predicted)\n```\n\\hfill\\break\n__Concordance__\n\\hfill\\break\nIdeally, the model-calculated-probability-scores of all actual Positive’s, (aka Ones) should be greater than the model-calculated-probability-scores of ALL the Negatives (aka Zeroes). Such a model is said to be perfectly concordant and a highly reliable one. This phenomenon can be measured by Concordance and Discordance.\n\nIn simpler words, of all combinations of 1-0 pairs (actuals), Concordance is the percentage of pairs, whose scores of actual positive’s are greater than the scores of actual negative’s. For a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of model. This model has a concordance of 96.8%.\n```{r}\nConcordance(testData$index_now, predicted)\n```\n\\hfill\\break\n__Specificity and Sensitivity__\n\\hfill\\break\n- Sensitivity (or True Positive Rate) is the percentage of 1’s (actuals) correctly predicted by the model, while, specificity is the percentage of 0’s (actuals) correctly predicted. In this model, it was found to be 86.29%.\n\\hfill\\break\n- Specificity can also be calculated as 1 - False Positive Rate. In this model, it was found to be 98.19%. \n```{r}\nsensitivity(testData$index_now, predicted, threshold = optCutOff)\nspecificity(testData$index_now, predicted, threshold = optCutOff)\n```\n\\hfill\\break\n__Confusion Matrix__\n\\hfill\\break\nIn the confusion matrix, the columns are actuals, while rows are predicteds. \n```{r}\nconfusionMatrix(testData$index_now, predicted, threshold = optCutOff)\n```\n",
    "created" : 1509430780773.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4138984762",
    "id" : "26644F20",
    "lastKnownWriteTime" : 1509756545,
    "last_content_update" : 1509756545119,
    "path" : "~/thesis_final/index/04-conclusion.Rmd",
    "project_path" : "index/04-conclusion.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}