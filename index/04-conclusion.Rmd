# Logistic Regression Model 
Once the final data set was created and cleaned, with a number of response variables including trailing beta, trailing volatility, price to book ratio, and the whether or not a stock was in the Minimum Volatility index or not (1 if in, 0 if not in). Given the nature of the data, a logistic regression was run. Looking at all of the historical data and stock various characteristics, this  modeled the log odds of a stock being in the minimum volatility index as a combination of the linear predictors mentioned. Since the index is rebalanced twice a year, it makes sense to look at a model including the data for both of these months.

## Data Cleaning
```{r, echo = FALSE}
load("~/thesis_final/data/monthly_data.Rda")
# Subset data for dates from May
may_final <- filter(monthly_data, date == "2012-05-31" |  date == "2013-05-31"| date == "2014-05-30" | date == "2015-05-29" | date == "2016-05-31")
# Remove NA values from set
may_final <- subset(may_final, !is.na(index_before))
# Subset data for dates from November
november_final <- filter(monthly_data, date == "2011-11-30" | date == "2012-11-30"| date == "2013-11-29"| date == "2014-11-28" | date == "2015-11-30" | date == "2016-11-30")
# Remove NA values from set
november_final <- subset(november_final, !is.na(index_before))

both_final <- rbind(may_final, november_final)
```
### Removing Class Bias
Ideally, the proportion of stocks in and out of the Minimum Volatility index should approximately be the same. However, after checking this, it is clear that this is not the case, as there were 4182 observations for stocks not in the index, and just 1385 observations for stocks in the index. Since just around 25% of the data was from stocks that are currently in the index, there is class bias. As a result, observations were sampled in equal proportions to get a better model.
```{r, cache=FALSE, echo = FALSE}
bias1<- table(both_final$index_now)
```
### Creating Development and Test Samples
One way to address the problem of class bias is to draw the 0’s and 1’s for the development sample in equal proportions for a majority of the observations. In doing so, the remaining data will not included for the model, but will be used as the validation sample to evaluate and test the model.  Class bias was removed, as the sample is now evenly weighted, with each outcome being represented by 969 observations. The remainder of the data was placed in the validation sample. 

```{r, cache=FALSE, echo=FALSE}
# Create Training Data
input_ones2 <- both_final[which(both_final$index_now == 1), ] 
input_zeros2 <- both_final[which(both_final$index_now == 0), ]
set.seed(100)  # for repeatability of samples
input_ones_training_rows2 <- sample(1:nrow(input_ones2), 0.7*nrow(input_ones2)) 
input_zeros_training_rows2 <- sample(1:nrow(input_zeros2), 0.7*nrow(input_ones2))  
training_ones2 <- input_ones2[input_ones_training_rows2, ]  
training_zeros2 <- input_zeros2[input_zeros_training_rows2, ]
trainingData <- rbind(training_ones2, training_zeros2)   
# Create Test Data
test_ones2 <- input_ones2[-input_ones_training_rows2, ]
test_zeros2 <- input_zeros2[-input_zeros_training_rows2, ]
testData <- rbind(test_ones2, test_zeros2) 
```

```{r, cache=FALSE, echo=FALSE}
bias2<- table(trainingData$index_now)
```

## Logistic Regression Model 
```{r, echo=FALSE}
# Model
logit <- glm(index_now ~  volatility + beta + price_to_book 
+ index_before, data=trainingData, family=binomial(link="logit"))

# Summary of Model
summary(logit)
```

```{r, cache=FALSE, echo=FALSE}
# Coefficient Interpretation
## Log Odds
# exp(coef(logit))
## Probability 
# (exp(coef(logit))) / (1+(exp(coef(logit))))
``` 

### Model Interpretation
The coefficients can be interpreted as: 
\hfill\break
- Volatility: The odds ratio of being added to the index is 0.96 times smaller, given a one unit increase in volatility. This response variable is not statistically significant.
\hfill\break
- Beta: The odds ratio of being added to the index is 0.015 times smaller, given a one unit increase in beta. This response variable is statistically significant. 
\hfill\break
- Price to Book: The odds ratio of being added to the index is the same, given a one unit increase in price to book ratio. This response variable is not statistically significant. 
\hfill\break
- Index before: The odds ratio of being added to the index is 228.61 times greater if the stock was in the index 6 months ago. This response variable is statistically significant. 
\hfill\break

### Diagnostic Testing 
To test the quality of the model, several diagnostic tests were performed:
\hfill\break
__Variance Inflation Factors__
\hfill\break
Variance inflation factors (VIF) are used in regressions to check for multicollinearity, which occurs when one predictor variable can be predicted from the others with a high degree of accuracy. An example of this includes a person's height and weight, as these two variables are very closely related. VIFs measure how much the variance of the estimated regression coefficients in the model are inflated, compared to a case when the predictor variables are linearly unrelated. The general rule of thumb is that predictor variables with a VIF over 4 should be further inspected, and variables with a VIF above 10 require correction. As shown below, all the predictor variables in the model have VIFs well below 4, indicating that multicollinearity is not a concern:
```{r, echo=FALSE, message=FALSE}
library(car)
vif(logit)
```
\hfill\break
__Optimal Cutoff__
\hfill\break
To get a sense of the model's predictive power, it is important to first tune the optimal cutoff to improve the model by reducing misclassification error. The default cutoff is often 0.5, but by tuning the probability cutoff, the ability of the model to correctly predict 0's and 1's greatly improves. In this model, that cutoff was found to be 0.86. 
```{r, cache=FALSE, echo=FALSE}
library(InformationValue)
predicted <- plogis(predict(logit, testData)) 
optCutOff <- optimalCutoff(testData$index_now, predicted)[1] 
```
\hfill\break
__Concordant and Discordant Pairs__
\hfill\break
To calculate the number of concordant and discordant pairs, all the possible pairs of 0's and 1's will be considered - that is, all the pairs of observations for stocks not in the index, and stocks in the index. In this data set, a total of 1,336,608 pairs were considered, and the probability values for 0 and 1 were calculated for each. Thus, considering each pair, which can be represented as (1,0), ranges for cutoff values were determined based off of the probability of each observed 0 and 1. Concordant pairs were noted when the cutoff value for the 1 was higher than the cutoff value of the 0. Discordant pairs were noted when this was not the case, and this was equivalent to the case where the probability of the event was lower than the probability of a non-event. These instances are contrary to the definition of the event and non-event based off of the cutoff value calculated. In a perfect model, 100% of the pairs would be concordant. In this model, 96.8% of the pairs were concordant and 3.2% of the pairs were discordant.
```{r, cache=FALSE, echo=FALSE}
concord1 <- Concordance(testData$index_now, predicted)
```
\hfill\break
__Misclassification Error__
\hfill\break
Misclassification error measures the accuracy of the model by determining what the percent mismatch is of the predicted outcomes compared to the actual outcomes. By looking at the test data, this quantifies how accurate the model was at predicting which stocks were in the index and what stocks were not in the index, given their predictor variable values. Better models will have lower misclassification errors. In this logistic regression model, the misclassification error was 4.02%. 
```{r, cache=FALSE, echo=FALSE}
miscl1<- misClassError(testData$index_now, predicted)
```
\hfill\break
__Sensitivity and Specificity__
\hfill\break
Sensitivity, also called the true positive rate, is the proportion of stocks in the minimum volatility index that were correctly predicted by the model. In the logistic regression model, the sensitivity was found to be 86.29%. Specificity, also called the true negative rate, is the proportion of stocks not in the minimum volatility index that were correctly predicted by the model, and can be calculated as 1 - False Positive Rate. In the logistic regression model, the specificity was found to be 98.19%.
```{r, cache=FALSE, echo=FALSE}
sens1<- sensitivity(testData$index_now, predicted, threshold = optCutOff)
spec1<- specificity(testData$index_now, predicted, threshold = optCutOff)
```
\hfill\break
__Confusion Matrix__
\hfill\break
The confusion matrix is a table showing the total number of true positives, true negatives, false positives, and false negatives from the test data to evaluate the performance of a model. In this logistic regression model, there were a total of 359 true positives, 3155 true negatives, 58 false negatives, and 57 false positives. Shown below, the columns are actuals outcomes, while the rows are predicted outcomes: 
```{r fig.cap = paste("Regression Model Confusion Matrix"), echo=FALSE}
matrix1<- confusionMatrix(testData$index_now, predicted, threshold = optCutOff)

kable(matrix1, 
     # col.names = c("Rank", "Ticker", "Average Weight"),
      caption = "Regression Model Confusion Matrix",
      longtable = TRUE,
      booktabs = TRUE)
```
\hfill\break
__Receiver Operating Characteristics Curve__
\hfill\break
The Receiver Operating Characteristics (ROC) Curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) for all different possible cutoffs. The area under the curve is a measure of the accuracy of the model in distinguishing between stocks in the index and stocks not in the index. A perfect model would yield a ROC curve passing through the top left corner, representing a sensitivity of 100% and a specificity of 100%, with an area under the curve of 100%. The area under the ROC curve for the logistic regression model in this study is 96.77%:
```{r fig.cap = paste("ROC Curve for Logistic Regression Model"), echo=FALSE}
plotROC(testData$index_now, predicted)
```
\hfill\break

