```{r include_packages_2, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("bookdown", repos = "http://cran.rstudio.com")
if(!require(thesisdown)){
  library(devtools)
  devtools::install_github("ismayc/thesisdown")
  }
library(thesisdown)
flights <- read.csv("data/flights.csv")
```

# Data Analysis
## Changes in Largest Holdings by Average Weight by Index
The top 5 largest holdings in the US Equal Weight Index by average weight are Apple, Exxon Mobil, Microsoft, General Electric, and Johnson & Johnson.

```{r, echo = FALSE}
load("~/thesis_final/data/usa.Rda")

usa_topweight <- aggregate(weight ~ ticker, data=usa, FUN=sum)
usa_topweight <- arrange(usa_topweight, desc(weight))
usa_topweight$weight <- usa_topweight$weight/63
usa_topweight$rank <- seq.int(nrow(usa_topweight))
```

```{r fig.cap = paste("Top 5 Holdings in US Equal Weight Index over Time by Average Weight"), echo=FALSE}

usa_topweight <- select(usa_topweight, rank, ticker, weight)
usa_topweight <- usa_topweight[1:5,]

kable(usa_topweight, 
      col.names = c("Rank", "Ticker", "Average Weight"),
      caption = "Top 5 Holdings in the US Equal Weight Index over Time by Average Weight",
      longtable = TRUE,
      booktabs = TRUE)
```

```{r, echo = FALSE}
load("~/thesis_final/data/usa.Rda")
load("~/thesis_final/data/minvol.Rda")

usa_sub1 <- filter(usa, ticker == "AAPL" | ticker == "XOM" | ticker == "MSFT" | ticker == "GE" | ticker == "JNJ")
usa_sub1 <- select(usa_sub1, date, ticker, weight)

library(ggplot2)

holdings1<- ggplot() + 
	geom_line(data = filter(usa_sub1, ticker == "AAPL"), aes(x = date, y = weight, color = "AAPL")) +
	geom_line(data = filter(usa_sub1, ticker == "XOM"), aes(x = date, y = weight, color = "XOM"))  +
	geom_line(data = filter(usa_sub1, ticker == "MSFT"), aes(x = date, y = weight, color = "MSFT"))  +
	geom_line(data = filter(usa_sub1, ticker == "GE"), aes(x = date, y = weight, color = "GE"))  +
	geom_line(data = filter(usa_sub1, ticker == "JNJ"), aes(x = date, y = weight, color = "JNJ"))  +
	xlab('date') + ylab('weight') + ggtitle('Average Weight Change of  Top 5 Holdings in the US Equal Weight Index')
```

``` {r fig.cap = paste("The 5 largest holdings of the US Equal Weight Index are AAPLE, XOM, MSFT, GE, and JNJ. The weights of the 5 companies start off very high, a few percent of the overall index, then suddenly all drop significantly to a fraction of a percent after 2015-08-31. After calling iShares and MSCI to verify this in the data, it was discovered this due to change in the weighting mechanism."), echo=FALSE}
plot(holdings1)
```

The top 5 largest holdings in the Minimum Volatility Index by average weight are Verizon, AT&T, Automatic Data Processing, Johnson & Johnson, and McDonald's. 

```{r, echo = FALSE}
load("~/thesis_final/data/minvol.Rda")

minvol_topweight <- aggregate(weight ~ ticker, data=minvol, FUN=sum)
minvol_topweight <- arrange(minvol_topweight, desc(weight))
minvol_topweight$weight <- minvol_topweight$weight/63
minvol_topweight$rank <- seq.int(nrow(minvol_topweight))
```

```{r fig.cap = paste("Top 5 Holdings in Minimum Volatility Index over Time by Average Weight"), echo=FALSE}

minvol_topweight <- select(minvol_topweight, rank, ticker, weight)
minvol_topweight <- minvol_topweight[1:5,]

kable(minvol_topweight, 
      col.names = c("Rank", "Ticker", "Average Weight"),
      caption = "Top 5 Holdings in the Minimum Volatility Index over Time by Average Weight",
      longtable = TRUE,
      booktabs = TRUE)
```

```{r, echo = FALSE}
load("~/thesis_final/data/minvol.Rda")
minvol_sub1 <- filter(minvol, ticker == "VZ" | ticker == "T" | ticker == "ADP" | ticker == "JNJ" | ticker == "MCD")
minvol_sub1 <- select(minvol_sub1, date, ticker, weight)

library(ggplot2)

holdings2<- ggplot() + 
	geom_line(data = filter(minvol_sub1, ticker == "VZ"), aes(x = date, y = weight, color = "VZ")) +
	geom_line(data = filter(minvol_sub1, ticker == "T"), aes(x = date, y = weight, color = "T"))  +
	geom_line(data = filter(minvol_sub1, ticker == "ADP"), aes(x = date, y = weight, color = "ADP"))  +
	geom_line(data = filter(minvol_sub1, ticker == "JNJ"), aes(x = date, y = weight, color = "JNJ"))  +
	geom_line(data = filter(minvol_sub1, ticker == "MCD"), aes(x = date, y = weight, color = "MCD"))  +
	xlab('date') + ylab('weight') + ggtitle('Average Weight Change of  Top 5 Holdings in the Minimum Volatility Index')
```

``` {r fig.cap = paste("The 5 largest holdings of the Minimum Volatility Index are VZ, T, ADP, JNJ, and MCD. Since the index's inception, these stocks have generally remained between a weight of 1.3% and 1.7% of the overall portfolio, with the exception of Verizon, which reached around 2% in 2014."), echo=FALSE}
plot(holdings2)
```

## Sector Weights
Sector weights were calculated over time for both the US Equal Weight and the Minimum Volatility Index. This was done to get a sense of what industries may be inherently more "low-risk" as well as checking to make sure the data is resilient, and each sector weighting in the Minimum Volatility Index is within 5% of the US Equal Weight Index, as specified by the Barra Optimizer. 

```{r, echo=FALSE}
load("~/thesis_final/data/usa_percent.Rda")
load("~/thesis_final/data/minvol_percent.Rda")
library(ggplot2)
```

``` {r fig.cap = paste("The weighting of energy in the US Equal Weight Index is consistenty higher than the weighting in the Minimum Volatility Index. This might imply that energy is a more volatile industry when compared to others."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Energy
Eng1 <- usa_percent[which(usa_percent$sector_name=="Energy"), ]
Eng2 <- minvol_percent[which(minvol_percent$sector_name=="Energy"), ]
ggplot(Eng1, aes(date, percent, colour = "US Equal Weight")) + geom_line() +  
ggtitle("Index Energy Sector Weight Comparisons Over Time") + xlab("Time") + ylab("Sector Weight") +
geom_line(data = Eng2, aes(x=date, y=percent, colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weighting of financials in the US Equal Weight Index are pretty consistent until a significant decrease towards the latter half of 2016. In the Minimum Volatility Index, the weighting of financials fluctuate quite a bit, and experience a similar drop in weight at the same time as the US Equal Weight Index."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Financials
Fin1 <- usa_percent[which(usa_percent$sector_name=="Financials"), ]
Fin2 <- minvol_percent[which(minvol_percent$sector_name=="Financials"), ]
ggplot(Fin1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Financial Sector Weight Comparisons Over Time")+xlab("Time")+ylab("Sector Weight") + 
geom_line(data = Fin2, aes(x=date, y=percent, colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weighting of consumer staples in the US Equal Weight Index is consistenty lower than the weighting of consumer staples in the Minimum Volatility Index. This might imply that consumer staples is a low volatile industry."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Consumer Staples
ConStap1 <- usa_percent[which(usa_percent$sector_name=="Consumer Staples"), ]
ConStap2 <- minvol_percent[which(minvol_percent$sector_name=="Consumer Staples"), ]
ggplot(ConStap1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Consumer Staples Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = ConStap2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of consumer discretionary stocks is consistenty higher in the US Equal Weight Index than in the Minimum Volatility Index. This might imply that consumer discretionary is inherently a volatile industry."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Consumer Discretionary
ConDis1 <- usa_percent[which(usa_percent$sector_name=="Consumer Discretionary"), ]
ConDis2 <- minvol_percent[which(minvol_percent$sector_name=="Consumer Discretionary"), ]
ggplot(ConDis1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Consumer Discretionary Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = ConDis2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of healthcare stocks is consistenty higher over time in the Minimum Volatility Index than in the US Equal Weight Index. This could imply that the healthcare sector may not be as volatile as other sectors."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Healthcare
Health1 <- usa_percent[which(usa_percent$sector_name=="Health Care"), ]
Health2 <- minvol_percent[which(minvol_percent$sector_name=="Health Care"), ]
ggplot(Health1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Healthcare Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = Health2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of industrials is consistenty higher over time in the US Equal Weight Index than in the Minimum Volatility Index. This could imply that industrial companies may be more volatile than companies in other sectors."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Industrials
Ind1 <- usa_percent[which(usa_percent$sector_name=="Industrials"), ]
Ind2 <- minvol_percent[which(minvol_percent$sector_name=="Industrials"), ]
ggplot(Ind1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Industrials Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = Ind2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weighting of information technology stocks between 2012 and 2013 were significantly greater in the Minimum Volatility Index than in the US Equal Weight Index. However, since 2014, the weights of IT in both indices have converged, implying the industry may have more volatile over the last few years."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Information Technology
IT1 <- usa_percent[which(usa_percent$sector_name=="Information Technology"), ]
IT2 <- minvol_percent[which(minvol_percent$sector_name=="Information Technology"), ]
ggplot(IT1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Information Technology Sector Weight Comparisons Over Time") + 
xlab("Time") + ylab("Sector Weight") + geom_line(data = IT2, aes(x=date, y=percent,
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of materials is consistenty higher over time in the US Equal Weight Index than in the Minimum Volatility Index. This could imply that materials companies may be more volatile than companies in other sectors."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Materials
Mat1 <- usa_percent[which(usa_percent$sector_name=="Materials"), ]
Mat2 <- minvol_percent[which(minvol_percent$sector_name=="Materials"), ]
ggplot(Mat1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Materials Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = Mat2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of utilities stocks is higher over time in the Minimum Volatility Index than in the US Equal Weight Index. This could imply that the utilities sector may not be as volatile as other sectors."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Utilites
Util1 <- usa_percent[which(usa_percent$sector_name=="Utilities"), ]
Util2 <- minvol_percent[which(minvol_percent$sector_name=="Utilities"), ]
ggplot(Util1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Utilites Sector Weight Comparisons Over Time") + xlab("Time") + 
ylab("Sector Weight") + geom_line(data = Util2, aes(x=date, y=percent, 
colour="Minimum Volatility"),show.legend = TRUE)
```

``` {r fig.cap = paste("The weight of telecommunications stocks is higher over time in the Minimum Volatility Index than in the US Equal Weight Index. However, over time the weight of telecommunications has consistently decreased, which would imply the industry has gotten more volatile over the past few years."), fig.height=3.5, fig.width=6.5, echo=FALSE}
## Telecommunication Services
Telecom1 <- usa_percent[which(usa_percent$sector_name=="Telecommunications"), ]
Telecom2 <- minvol_percent[which(minvol_percent$sector_name=="Telecommunications"), ]
ggplot(Telecom1, aes(date, percent, colour = "US Equal Weight")) + geom_line() + 
ggtitle("Index Telecommunication Sector Weight Comparisons Over Time") + 
xlab("Time") + ylab("Sector Weight") + geom_line(data = Telecom2, aes(x=date, 
y=percent, colour="Minimum Volatility"),show.legend = TRUE)
```

\hfill\break
 

## Trailing Volatilities 
Data was collected from 10/31/2010 to 12/30/2016, from Wharton Research Data Services (WRDS) for the 908 historical constituents of the USA Equal Weight (EUSA) ETF, of which the Minimum Volatility Index is derived. Each tickers' 252-day (annual) trailing volatility was calculated, and a month end spagetti plot was produced for stocks in the Minimum Volatility Index, and the remainder of the stock comprising the US Equal Weight Index, to get a relative sense of each group's volatility attributes. The stocks comprising the Minimum Volatility Index generally had a lower volatility than those in the US Equal Weight Index.

```{r, echo = FALSE}
library(ggplot2)
load("~/thesis_final/data/monthly_data.Rda")

usa_vol <-filter(monthly_data, index_now == 0)
summary(usa_vol$volatility)

us_volplot <- ggplot(usa_vol, aes(date, volatility, group = ticker)) + geom_line() + ggtitle("52-Week Trailing Volatility for Stocks in the US Equal Weight Index") + labs(x="Date", y="52-week Trailing Volatility")
```

``` {r fig.cap = paste("The 52-week trailing volatilities for stocks in the US Equal Weight Index ranged from 0.00492 to 87.37030, with a mean value of 1.44738."), echo=FALSE}
plot(us_volplot)
```

```{r, echo = FALSE}
library(ggplot2)
load("~/thesis_final/data/monthly_data.Rda")

minvol_vol <-filter(monthly_data, index_now == 1)
summary(minvol_vol$volatility)

minvol_volplot <- ggplot(minvol_vol, aes(date, volatility, group = ticker)) + geom_line() + ggtitle("52-Week Trailing Volatility for Stocks in the Minimum Volatility Index") + labs(x="Date", y="52-week Trailing Volatility")
```

``` {r fig.cap = paste("The 52-week trailing volatilities for stocks in the Minimum Volatility Index ranged from 0.03085 to 34.20974, with a mean value of 1.40784."), echo=FALSE}
plot(minvol_volplot)
```

## Trailing Betas 
Data was collected from 10/31/2010 to 12/30/2016, from Wharton Research Data Services (WRDS) for the 908 historical constituents of the USA Equal Weight (EUSA) ETF, of which the Minimum Volatility Index is derived. Each tickers' 252-day (annual) trailing beta was calculated, and a month end spagetti plot was produced for stocks in the Minimum Volatility Index, and the remainder of the stock comprising the US Equal Weight Index, to get a relative sense of each group's volatility attributes. Generally, the stocks comprising the Minimum Volatility Index had a lower beta than those in the US Equal Weight Index.

```{r, echo = FALSE}
library(ggplot2)
load("~/thesis_final/data/monthly_data.Rda")

usa_beta <-filter(monthly_data, index_now == 0)
summary(usa_beta$beta)

usa_beta_plot <- ggplot(usa_beta, aes(date, beta, group = ticker)) + geom_line() + ggtitle("52-Week Trailing Beta for Stocks in the US Equal Weight Index") + labs(x="Date", y="52-week Trailing Beta")
```

``` {r fig.cap = paste("The 52-week trailing betas for stocks in the US Equal Weight Index ranged from -4.9037 to 6.4952, with a mean value of 1.1505"), echo=FALSE}
plot(usa_beta_plot)
```

```{r, echo = FALSE}
library(ggplot2)
load("~/thesis_final/data/monthly_data.Rda")

minvol_beta <-filter(monthly_data, index_now == 1)
summary(minvol_beta$beta)

minvol_beta_plot <- ggplot(minvol_beta, aes(date, beta, group = ticker)) + geom_line() + ggtitle("52-Week Trailing Beta for Stocks in the Minimum Volatility Index") + labs(x="Date", y="52-week Trailing Beta")
```

``` {r fig.cap = paste("The 52-week trailing betas for stocks in the Minimum Volatility Index ranged from -0.2473 to 3.2021, with a mean value of 0.7856"), echo=FALSE}
plot(minvol_beta_plot)
```


# Model 
Once the final data set was created and cleaned, with a number of response variables including trailing beta, trailing volatility, price to book ratio, and the whether or not a stock was in the Minimum Volatility index or not (1 if in, 0 if not in). Given the nature of the data, a logistic regression was run. Looking at all of the historical data and stock various characteristics, this  modeled the log odds of a stock being in the minimum volatility index as a combination of the linear predictors mentioned. Several models were ran, including one by certain months, and one by the entire pool of data, and the best one was chosen to test in further detail.

## Model 1: May Model 
Since the index is rebalanced twice a year, one of the times in May, it makes sense to look at a model specifically for this month. 

```{r, echo = FALSE}
# Subset data for dates from May only
may_final <- filter(monthly_data, date == "2012-05-31" |  date == "2013-05-31"| date == "2014-05-30" | date == "2015-05-29" | date == "2016-05-31")
# Remove NA values from set
may_final <- subset(may_final, !is.na(index_before))
```
### Removing Class Bias
Ideally, the proportion of stocks in and out of the Min Vol index should approximately be the same. However, after checking this, it is clear that this is not the case, as, just around 25% of the data is from stocks that are currently in the index, meaning there is a class bias. As a result, observations must be sampled in approximately equal proportions to get a better model.
```{r}
table(may_final$index_now)
```
### Creating Development and Validation Data
One way to address the problem of class bias is to draw the 0’s and 1’s in equal proportions, and using that development sample for the model. The unsampled data will be kept as validation sample to test the model later on. As a result, the size of development sample will be smaller that validation sample, which is not a problem, since there are large number of observations.
```{r, echo=FALSE}
# Create Training Data
input_ones1 <- may_final[which(may_final$index_now == 1),]  
input_zeros1 <- may_final[which(may_final$index_now == 0),]
set.seed(100)
input_ones_training_rows1 <- sample(1:nrow(input_ones1), 0.7*nrow(input_ones1)) 
input_zeros_training_rows1 <- sample(1:nrow(input_zeros1), 0.7*nrow(input_ones1))
training_ones1 <- input_ones1[input_ones_training_rows1, ]  
training_zeros1 <- input_zeros1[input_zeros_training_rows1, ]
trainingData1 <- rbind(training_ones1, training_zeros1)   
# Create Test Data
test_ones1 <- input_ones1[-input_ones_training_rows1, ]
test_zeros1 <- input_zeros1[-input_zeros_training_rows1, ]
testData1 <- rbind(test_ones1, test_zeros1) 
```
There is no more class bias, as the sample is now evenly weighted now, with each outcome being represented by 468 observations. 
```{r}
table(trainingData1$index_now)
```

### Logistic Regression Model 
```{r}
# Model 1
logit1 <- glm(index_now ~  volatility + beta + price_to_book + 
index_before, data=trainingData1, family=binomial(link="logit"))

# Summary of Model 1
summary(logit1)

# Coefficient Interpretation
## Log Odds
exp(coef(logit1))
## Probability 
(exp(coef(logit1))) / (1+(exp(coef(logit1))))
``` 

### Interpretation of May Model
The coefficients can be interpreted as: 
\hfill\break
- Volatility: The odds ratio of being added to the index is 0.95 times smaller, given a one unit increase in volatility. This response variable is not statistically significant.
\hfill\break
- Beta: The odds ratio of being added to the index is 0.013 times smaller, given a one unit increase in beta. This response variable is statistically significant. 
\hfill\break
- Price to Book: The odds ratio of being added to the index is 1.00 times smaller, given a one unit increase in price to book ratio. This response variable is not statistically significant. 
\hfill\break
- Index before: The odds ratio of being added to the index is 682.24 times greater if the stock was in the index 6 months ago. This response variable is statistically significant. 
\hfill\break
\hfill\break

### Model Quality 
To test the quality of the model, several tests were done:
\hfill\break
__Predictive Power__
\hfill\break
The default cutoff prediction probability score is 0.5 or the ratio of 1’s and 0’s in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The InformationValue::optimalCutoff function provides ways to find the optimal cutoff to improve the prediction of 1’s, 0’s, both 1’s and 0’s and to reduce the misclassification error. Here, the optimal cut off is 0.85.
```{r}
library(InformationValue)
predicted1 <- plogis(predict(logit1, testData1)) 
optCutOff1 <- optimalCutoff(testData1$index_now, predicted1)[1] 
optCutOff1
```
\hfill\break
__VIF__
\hfill\break
Like in case of linear regression, we should check for multicollinearity in the model. As seen below, all of the variables in the model have VIF well below 4.
```{r, message=FALSE}
library(car)
vif(logit1)
```
\hfill\break
__Misclassification Error__
\hfill\break
Misclassification error is the percentage mismatch of predicted vs actuals, irrespective of 1’s or 0’s. The lower the misclassification error, the better the model. Here it is 2.3%.
```{r}
predicted1 <- plogis(predict(logit1, testData1)) 
misClassError(testData1$index_now, predicted1)
```
\hfill\break
__ROC__
\hfill\break
Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. For a good model, as the cutoff is lowered, it should mark more of actual 1’s as positives and lesser of actual 0’s as 1’s. So for a good model, the curve should rise steeply, indicating that the TPR (Y-Axis) increases faster than the FPR (X-Axis) as the cutoff score decreases. Greater the area under the ROC curve, better the predictive ability of the model. Here, it is 98.19%.
```{r}
plotROC(testData1$index_now, predicted1)
```
\hfill\break
__Concordance__
\hfill\break
Ideally, the model-calculated-probability-scores of all actual Positive’s, (aka Ones) should be greater than the model-calculated-probability-scores of ALL the Negatives (aka Zeroes). Such a model is said to be perfectly concordant and a highly reliable one. This phenomenon can be measured by Concordance and Discordance.

In simpler words, of all combinations of 1-0 pairs (actuals), Concordance is the percentage of pairs, whose scores of actual positive’s are greater than the scores of actual negative’s. For a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of model. This model has a concordance of 98.2%.
```{r}
Concordance(testData1$index_now, predicted1)
```
\hfill\break
__Specificity and Sensitivity__
\hfill\break
- Sensitivity (or True Positive Rate) is the percentage of 1’s (actuals) correctly predicted by the model, while, specificity is the percentage of 0’s (actuals) correctly predicted. In this model, it was found to be 90.6%.
\hfill\break
- Specificity can also be calculated as 1 - False Positive Rate. In this model, it was found to be 98.6%. 
```{r}
sensitivity(testData1$index_now, predicted1, threshold = optCutOff1)
specificity(testData1$index_now, predicted1, threshold = optCutOff1)
```
\hfill\break
__Confusion Matrix__
\hfill\break
In the confusion matrix, the columns are actuals, while rows are predicteds. 
```{r}
confusionMatrix(testData1$index_now, predicted1, threshold = optCutOff1)
```

## Model 2: Total Rebalancing (November & May) Model 
Since the index is rebalanced twice a year, it makes sense to look at a model including the data for both of these months.

```{r, echo=FALSE}
# Subset data for dates from May and November
november_final <- filter(monthly_data, date == "2011-11-30" | date == "2012-11-30"| date == "2013-11-29"| date == "2014-11-28" | date == "2015-11-30" | date == "2016-11-30")
# Remove NA values from set
november_final <- subset(november_final, !is.na(index_before))

both_final <- rbind(may_final, november_final)
```
### Removing for Class Bias
Ideally, the proportion of stocks in and out of the Min Vol index should approximately be the same. However, after checking this, it is clear that this is not the case, as, just around 25% of the data is from stocks that are currently in the index, meaning there is a class bias. As a result, observations must be sampled in approximately equal proportions to get a better model.
```{r}
table(both_final$index_now)
```
### Create Training and Test Samples
One way to address the problem of class bias is to draw the 0’s and 1’s for the trainingData (development sample) in equal proportions. In doing so, we will put rest of the inputData not included for training into testData (validation sample). As a result, the size of development sample will be smaller that validation, which is okay, because, there are large number of observations.
```{r, echo=FALSE}
# Create Training Data
input_ones2 <- both_final[which(both_final$index_now == 1), ] 
input_zeros2 <- both_final[which(both_final$index_now == 0), ]
set.seed(100)  # for repeatability of samples
input_ones_training_rows2 <- sample(1:nrow(input_ones2), 0.7*nrow(input_ones2)) 
input_zeros_training_rows2 <- sample(1:nrow(input_zeros2), 0.7*nrow(input_ones2))  
training_ones2 <- input_ones2[input_ones_training_rows2, ]  
training_zeros2 <- input_zeros2[input_zeros_training_rows2, ]
trainingData2 <- rbind(training_ones2, training_zeros2)   
# Create Test Data
test_ones2 <- input_ones2[-input_ones_training_rows2, ]
test_zeros2 <- input_zeros2[-input_zeros_training_rows2, ]
testData2 <- rbind(test_ones2, test_zeros2) 
```
There is no more class bias, as the sample is now evenly weighted now, with each outcome being represented by 969 observations. 
```{r}
table(trainingData2$index_now)
```

### Logistic Regression Model 
```{r}
# Model 2
logit2 <- glm(index_now ~  volatility + beta + price_to_book 
+ index_before, data=trainingData2, family=binomial(link="logit"))

# Summary of Model 2
summary(logit2)

# Coefficient Interpretation
## Log Odds
exp(coef(logit2))
## Probability 
(exp(coef(logit2))) / (1+(exp(coef(logit2))))
``` 

### Interpretation of Model
The coefficients can be interpreted as: 
\hfill\break
- Volatility: The odds ratio of being added to the index is 0.96 times smaller, given a one unit increase in volatility. This response variable is not statistically significant.
\hfill\break
- Beta: The odds ratio of being added to the index is 0.015 times smaller, given a one unit increase in beta. This response variable is statistically significant. 
\hfill\break
- Price to Book: The odds ratio of being added to the index is 0.50 times smaller, given a one unit increase in price to book ratio. This response variable is not statistically significant. 
\hfill\break
- Index before: The odds ratio of being added to the index is 228.61 times greater if the stock was in the index 6 months ago. This response variable is statistically significant. 
\hfill\break
\hfill\break

### Model Quality 
To test the quality of the model, several tests were done:
\hfill\break
__Predictive Power__
\hfill\break
The default cutoff prediction probability score is 0.5 or the ratio of 1’s and 0’s in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The InformationValue::optimalCutoff function provides ways to find the optimal cutoff to improve the prediction of 1’s, 0’s, both 1’s and 0’s and to reduce the misclassification error. Here, the optimal cut off is 0.86.
```{r}
library(InformationValue)
predicted2 <- plogis(predict(logit2, testData2)) 
optCutOff2 <- optimalCutoff(testData2$index_now, predicted2)[1] 
optCutOff2
```
\hfill\break
__VIF__
\hfill\break
Like in case of linear regression, we should check for multicollinearity in the model. As seen below, all the variables in the model have VIF well below 4.
```{r, message=FALSE}
library(car)
vif(logit2)
```
\hfill\break
__Misclassification Error__
\hfill\break
Misclassification error is the percentage mismatch of predicted vs. actuals, irrespective of 1’s or 0’s. The lower the misclassification error, the better the model. Here it is 4.0%.
```{r}
misClassError(testData2$index_now, predicted2)
```
\hfill\break
__ROC__
\hfill\break
Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. For a good model, as the cutoff is lowered, it should mark more of actual 1’s as positives and lesser of actual 0’s as 1’s. So for a good model, the curve should rise steeply, indicating that the TPR (Y-Axis) increases faster than the FPR (X-Axis) as the cutoff score decreases. Greater the area under the ROC curve, better the predictive ability of the model. Here, it is 96.77%.
```{r}
plotROC(testData2$index_now, predicted2)
```
\hfill\break
__Concordance__
\hfill\break
Ideally, the model-calculated-probability-scores of all actual Positive’s, (aka Ones) should be greater than the model-calculated-probability-scores of ALL the Negatives (aka Zeroes). Such a model is said to be perfectly concordant and a highly reliable one. This phenomenon can be measured by Concordance and Discordance.

In simpler words, of all combinations of 1-0 pairs (actuals), Concordance is the percentage of pairs, whose scores of actual positive’s are greater than the scores of actual negative’s. For a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of model. This model has a concordance of 96.8%.
```{r}
Concordance(testData2$index_now, predicted2)
```
\hfill\break
__Specificity and Sensitivity__
\hfill\break
- Sensitivity (or True Positive Rate) is the percentage of 1’s (actuals) correctly predicted by the model, while, specificity is the percentage of 0’s (actuals) correctly predicted. In this model, it was found to be 86.29%.
\hfill\break
- Specificity can also be calculated as 1 - False Positive Rate. In this model, it was found to be 98.19%. 
```{r}
sensitivity(testData2$index_now, predicted2, threshold = optCutOff2)
specificity(testData2$index_now, predicted2, threshold = optCutOff2)
```
\hfill\break
__Confusion Matrix__
\hfill\break
In the confusion matrix, the columns are actuals, while rows are predicteds. 
```{r}
confusionMatrix(testData2$index_now, predicted2, threshold = optCutOff2)
```
